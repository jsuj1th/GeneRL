# Army Concentration Penalty Implementation

## ðŸŽ¯ Overview

Added **army concentration penalty** to PPO training to encourage better army distribution and more active exploration. This addresses the common RL problem where agents hoard armies on one tile (usually the general) instead of spreading them strategically.

**Implementation Date**: December 7, 2024  
**File Modified**: `src/training/train_ppo_potential.py`  
**Training Strategy**: Continue from existing checkpoint (Episode 100, 270K steps, 70% win rate)

---

## ðŸš€ What Was Changed

### **New Method: `compute_army_concentration_penalty()`**

Added comprehensive penalty calculation using three complementary metrics:

```python
def compute_army_concentration_penalty(self, observation):
    """
    Penalize uneven army distribution to encourage spreading armies.
    
    Returns:
        penalty: Value from -0.15 to +0.05
                 Negative = too concentrated
                 Positive = well distributed
    """
```

### **Integration Point**

Added to the rollout collection loop in `collect_rollout()`:

```python
# After computing shaped reward
shaped_reward = self.compute_shaped_reward(...)

# NEW: Add army concentration penalty
army_penalty = self.compute_army_concentration_penalty(next_observations[0])

total_reward = shaped_reward + army_penalty  # Combined reward
```

---

## ðŸ“Š Penalty Components

### **1. Coefficient of Variation (CV) Penalty**

**Purpose**: Penalize uneven distribution across all owned tiles

**Formula**:
```python
cv = std(armies) / mean(armies)
cv_penalty = -0.05 * min(cv, 3.0) / 3.0  # Range: 0 to -0.05
```

**Example**:
- Uniform [10, 10, 10, 10]: cv â‰ˆ 0 â†’ penalty = 0
- Concentrated [40, 0, 0, 0]: cv â‰ˆ 2.0 â†’ penalty = -0.033

**Why**: Coefficient of variation is scale-invariant, works regardless of total army size

---

### **2. Max Army Ratio Penalty**

**Purpose**: Penalize when one tile has disproportionately large army

**Formula**:
```python
max_ratio = max(armies) / mean(armies)
if max_ratio > 4.0:
    ratio_penalty = -0.05 * min((max_ratio - 4.0) / 6.0, 1.0)  # Max: -0.05
else:
    ratio_penalty = 0.0
```

**Example**:
- Tiles: [10, 10, 10, 100], mean=32.5, max=100, ratio=3.08 â†’ no penalty
- Tiles: [5, 5, 5, 200], mean=53.75, max=200, ratio=3.72 â†’ no penalty  
- Tiles: [5, 5, 5, 250], mean=66.25, max=250, ratio=3.77 â†’ no penalty
- Tiles: [3, 3, 3, 300], mean=77.25, max=300, ratio=3.88 â†’ no penalty
- Tiles: [2, 2, 2, 350], mean=89, max=350, ratio=3.93 â†’ no penalty
- Tiles: [1, 1, 1, 400], mean=100.75, max=400, ratio=3.97 â†’ no penalty
- Tiles: [1, 1, 1, 450], mean=113.25, max=450, ratio=3.97 â†’ no penalty
- Tiles: [1, 1, 1, 1, 1, 500], mean=84.17, max=500, ratio=5.94 â†’ penalty = -0.016

**Threshold**: Only penalize if max > 4Ã— average (allows some concentration for strategic reasons)

**Why**: Catches extreme cases where one tile has nearly all armies

---

### **3. Entropy Bonus**

**Purpose**: Reward even distribution using information theory

**Formula**:
```python
probs = armies / sum(armies)  # Normalize to probabilities
entropy = -sum(probs * log(probs))  # Information entropy
max_entropy = log(num_tiles)  # Maximum possible entropy
normalized_entropy = entropy / max_entropy  # 0 to 1
entropy_bonus = 0.05 * normalized_entropy  # Bonus: 0 to +0.05
```

**Example**:
- Uniform [10, 10, 10, 10]: entropy = log(4) â†’ bonus = +0.05
- Concentrated [40, 0, 0, 0]: entropy â‰ˆ 0 â†’ bonus â‰ˆ 0
- Mixed [20, 15, 10, 5]: entropy â‰ˆ 0.85Ã—log(4) â†’ bonus â‰ˆ +0.043

**Why**: 
- Entropy measures "evenness" of distribution
- High entropy = more uniform = better exploration
- Provides positive reinforcement (not just penalties)

---

## ðŸŽ¯ Total Penalty Range

**Combined Range**: -0.15 to +0.05

- **Best case** (uniform distribution): +0.05 (entropy bonus, no penalties)
- **Worst case** (very concentrated): -0.15 (all penalties apply)
- **Typical case**: -0.05 to 0.0

**Magnitude relative to other rewards**:
- Win reward: +10.0
- Loss penalty: -10.0
- Potential-based rewards: -2.0 to +2.0 per step
- Army penalty: -0.15 to +0.05 per step

The penalty is **small but persistent**, gradually encouraging better distribution without overwhelming the main objectives.

---

## ðŸ§  Why This Design Works

### **1. Multiple Complementary Metrics**

- CV catches overall unevenness
- Max ratio catches extreme outliers
- Entropy rewards good behavior (not just penalizing bad)

### **2. Scale-Invariant**

All metrics work regardless of:
- Total army size
- Number of tiles owned
- Stage of game (early/late)

### **3. Gradual Penalties**

Penalties scale smoothly:
- Small imbalance â†’ small penalty
- Large imbalance â†’ larger penalty
- No cliff effects or sudden jumps

### **4. Threshold-Based**

Max ratio only penalizes when ratio > 4.0:
- Allows some strategic concentration (building up for attacks)
- Only penalizes excessive hoarding
- Respects tactical decisions

---

## ðŸ“ˆ Expected Impact

### **Immediate Effects (Episodes 101-120)**

- âœ… Agent starts spreading armies more
- âœ… More tiles with active armies (not just general)
- âš ï¸ Win rate may dip slightly (10-20%) as agent adapts
- âš ï¸ Value network adjusts to new reward structure

### **Medium Term (Episodes 120-150)**

- âœ… Win rate recovers to ~70%
- âœ… Better exploration patterns emerge
- âœ… More diverse attack strategies
- âœ… Captures cities more actively

### **Long Term (Episodes 150+)**

- âœ… Win rate improves beyond 70%
- âœ… Max tiles explored increases (target: 30-50)
- âœ… Action diversity increases (target: 25-35%)
- âœ… More strategic gameplay (multiple fronts, coordinated attacks)

---

## ðŸ” How to Monitor

### **During Training**

Watch the logs for these indicators:

```
ðŸ“Š Episode Stats:
   Max Tiles: 25 â†’ 35 (improving exploration)
   Cities: 2 â†’ 4 (more active capture)
   Win Rate: 68% â†’ 74% (performance improving)
```

### **Using Monitor Script**

```bash
python src/evaluation/monitor_exploration.py --checkpoint_dir checkpoints/ppo_from_bc
```

Watch for:
- **Max Tiles Explored**: Should increase over time
- **Action Diversity**: Should increase (less repetitive moves)
- **Unique Actions**: Should increase (trying different strategies)

### **Visual Inspection**

```bash
./watch_my_agent.sh
```

Look for:
- âœ… Armies on multiple tiles (not just general)
- âœ… Active attacks from multiple directions
- âœ… Building armies on frontier tiles
- âœ… Strategic positioning before attacks
- âŒ All armies concentrated on general (bad - should reduce over time)

---

## ðŸŽ“ Theoretical Foundation

### **Potential-Based Reward Shaping**

The original training uses potential-based reward shaping (Ng et al., 1999):

```
r_shaped(s, a, s') = r(s, a, s') + Î³Ï†(s') - Ï†(s)
```

Where `Ï†(s)` is a potential function based on land, army, and cities.

### **Our Addition**

We add an **auxiliary reward** term:

```
r_total = r_shaped + r_army_concentration
```

This is **theoretically sound** because:

1. âœ… **Additive**: Doesn't change optimal policy if agent is optimal
2. âœ… **Small magnitude**: Doesn't overwhelm main objectives
3. âœ… **Aligned**: Spreading armies helps achieve main goals (exploration, capturing)
4. âœ… **Stationary**: Penalty doesn't change over episodes (unlike curriculum)

### **Relation to Exploration Bonuses**

Similar to:
- **Curiosity-driven exploration**: Reward visiting new states
- **Empowerment**: Reward maintaining options (spread armies = more options)
- **Diversity rewards**: Reward diverse behaviors

Our penalty encourages **strategic diversity** through army distribution.

---

## ðŸ”§ Hyperparameters

### **Current Settings**

```python
# CV Penalty
cv_scale = 0.05  # Max penalty from CV
cv_max = 3.0     # Clamp CV to this value

# Max Ratio Penalty  
ratio_threshold = 4.0  # Only penalize if max > 4Ã— mean
ratio_scale = 0.05     # Max penalty from ratio
ratio_range = 6.0      # Normalize (ratio - 4) by this

# Entropy Bonus
entropy_scale = 0.05  # Max bonus from entropy
```

### **Tuning Guidance**

If agent **still hoards armies** after 50 episodes:
- â†‘ Increase `cv_scale` to 0.07 or 0.10
- â†“ Lower `ratio_threshold` to 3.0
- â†‘ Increase `ratio_scale` to 0.07

If agent **spreads too thin** (weak attacks):
- â†“ Decrease `cv_scale` to 0.03
- â†‘ Increase `ratio_threshold` to 5.0
- Keep entropy bonus (rewards some concentration)

---

## ðŸ“‹ Implementation Checklist

- [x] âœ… Added `compute_army_concentration_penalty()` method
- [x] âœ… Integrated penalty into rollout collection
- [x] âœ… Uses three complementary metrics (CV, max ratio, entropy)
- [x] âœ… Penalty is additive to existing potential-based rewards
- [x] âœ… Magnitude is appropriate (-0.15 to +0.05)
- [x] âœ… Scale-invariant (works at any army size)
- [x] âœ… Tested with existing checkpoint (ready to resume)
- [x] âœ… Documentation complete

---

## ðŸš€ How to Use

### **Resume Training with New Penalty**

```bash
# Activate environment
source venv313/bin/activate

# Resume training (uses auto_resume by default)
python src/training/train_ppo_potential.py --auto_resume --training_hours 2.0
```

### **Monitor Progress**

```bash
# Terminal 1: Training
python src/training/train_ppo_potential.py --auto_resume --training_hours 4.0

# Terminal 2: Monitoring
python src/evaluation/monitor_exploration.py --checkpoint_dir checkpoints/ppo_from_bc
```

### **Watch Agent Play**

```bash
# After 20-30 more episodes
./watch_my_agent.sh
```

---

## ðŸ“Š Expected Timeline

| Episodes | Expectation | Metrics |
|----------|-------------|---------|
| 100 (current) | Baseline | Win: 70%, Tiles: ~15 |
| 101-110 | Adaptation period | Win: 60-65%, agent adjusting |
| 111-130 | Recovery | Win: 65-70%, better spreading |
| 131-150 | Improvement | Win: 70-75%, more exploration |
| 151-200 | Mastery | Win: 75-80%, strategic army use |

---

## ðŸŽ¯ Success Criteria

After 100 more episodes (total 200), we should see:

### **Exploration Metrics**
- âœ… Max tiles explored: 30-50 (vs current ~15)
- âœ… Action diversity: 25-35% (vs current ~17%)
- âœ… Unique actions per game: 40-60 (vs current ~30)

### **Strategic Metrics**
- âœ… Win rate: >75% vs BC opponent
- âœ… Cities captured: 3-5 per game
- âœ… Multiple attack fronts visible in gameplay
- âœ… General has <50% of total armies (spread effectively)

### **Behavioral Metrics**
- âœ… Armies on frontier tiles, not just general
- âœ… Coordinated multi-tile attacks
- âœ… Active city capture throughout game
- âœ… Reduced "camping" behavior

---

## ðŸ”¬ Alternative Approaches Considered

### **1. Frontier Army Reward**
Reward having armies on tiles adjacent to enemy/neutral territory.
- **Pros**: Directly encourages active positioning
- **Cons**: Requires expensive neighbor computation, may conflict with defensive play
- **Decision**: Simpler metrics (CV, entropy) achieve similar goal

### **2. Gini Coefficient**
Use Gini coefficient (economic inequality measure).
- **Pros**: Well-studied metric for inequality
- **Cons**: More complex computation, similar to CV
- **Decision**: CV + max ratio + entropy is simpler and more interpretable

### **3. Hard Constraint**
Force agent to never exceed 4Ã— concentration.
- **Pros**: Guaranteed spreading
- **Cons**: Breaks policy gradient, reduces flexibility
- **Decision**: Soft penalty allows learning and strategic exceptions

---

## ðŸ“š References

1. **Ng, A. Y., Harada, D., & Russell, S.** (1999). Policy invariance under reward transformations: Theory and application to reward shaping. *ICML*.

2. **Pathak, D., et al.** (2017). Curiosity-driven exploration by self-supervised prediction. *ICML*.

3. **Bellemare, M., et al.** (2016). Unifying count-based exploration and intrinsic motivation. *NIPS*.

4. **Eysenbach, B., et al.** (2018). Diversity is all you need: Learning skills without a reward function. *ICLR*.

---

## âœ… Status

**Implementation**: COMPLETE âœ…  
**Testing**: Ready for training âœ…  
**Monitoring**: Scripts available âœ…  
**Documentation**: Complete âœ…  

**Next Step**: Resume training and monitor for 20-30 episodes to see adaptation.

```bash
python src/training/train_ppo_potential.py --auto_resume --training_hours 2.0
```

ðŸš€ **Ready to train!**
