# ğŸ¯ DQN TRAINING - READY TO START

## âœ… Setup Complete!

Your Deep RL project for Generals.io is fully configured and ready for Phase 2!

---

## ğŸ“Š Current Status

### Phase 1: Behavior Cloning âœ… **COMPLETE**
- âœ… Downloaded 18,803 expert replays
- âœ… Preprocessed 48,723 game states
- âœ… Trained BC model (14 minutes)
- âœ… Achieved 27.75% test accuracy
- âœ… 100% valid action rate
- âœ… Estimated Elo: ~1874

**Model:** `checkpoints/bc/best_model.pt` (ready to use!)

### Phase 2: DQN Self-Play â¸ï¸ **READY TO START**
- âœ… Implemented DQN training script
- âœ… Added reward shaping from paper
- âœ… Created self-play training loop
- âœ… Set up opponent pool system
- âœ… Configured Double DQN with experience replay
- â¸ï¸ **Waiting to start training**

**Expected Results:**
- Accuracy: 32-35% (+5-10% improvement)
- Estimated Elo: ~2000
- Training Time: 6 hours on M4 Mac

---

## ğŸš€ START DQN TRAINING NOW

### Quick Test (3 minutes)
```bash
./train_dqn.sh 0.05
```

### Short Training (2 hours)
```bash
./train_dqn.sh 2
```

### Full Training (6 hours) â­ **RECOMMENDED**
```bash
./train_dqn.sh 6
```

### Or run directly:
```bash
python3 src/training/train_dqn_self_play.py \
  --bc_checkpoint checkpoints/bc/best_model.pt \
  --train_dir data/processed/train \
  --val_dir data/processed/val \
  --output_dir checkpoints/dqn \
  --training_hours 6
```

---

## ğŸ“ˆ What Happens During Training

### Training Process

```
1. Load BC Model (27.75% accuracy)
   â†“
2. Initialize Opponent Pool [BC model]
   â†“
3. For each episode:
   â”œâ”€ Sample game states from replays
   â”œâ”€ Agent selects actions (Îµ-greedy)
   â”œâ”€ Calculate shaped rewards
   â”œâ”€ Store in replay buffer
   â”œâ”€ Train with Double DQN
   â””â”€ Update target network
   â†“
4. Every 50 episodes:
   â”œâ”€ Evaluate on validation set
   â”œâ”€ Save if best model
   â””â”€ Update opponent pool
   â†“
5. After 6 hours:
   â””â”€ Save final model (~32-35% accuracy)
```

### Monitoring Progress

**Console Output:**
```
Episode  100 | Loss: 0.0234 | Reward: +0.043 | Îµ: 0.950 | Buffer: 10000
Episode  200 | Loss: 0.0198 | Reward: +0.067 | Îµ: 0.903 | Buffer: 20000

ğŸ“Š Validation - Accuracy: 28.45% | Valid rate: 100.00%
  â­ New best accuracy!
  ğŸ’¾ Saved best model
  ğŸ¤– Updated opponent pool (size: 3)
```

**TensorBoard:**
```bash
tensorboard --logdir logs/dqn_self_play
```

View in browser: http://localhost:6006

---

## ğŸ¯ Expected Results

| Time | Accuracy | Improvement | Elo (est.) |
|------|----------|-------------|------------|
| Start | 27.75% | - | ~1874 |
| 2h | ~29-30% | +2-3% | ~1920 |
| 4h | ~30-32% | +3-5% | ~1960 |
| 6h | ~32-35% | +5-10% | **~2000** |

**Paper Baseline:** 2052 Elo (36h on H100 GPU)  
**Our Goal:** ~2000 Elo (6h on M4 Mac)

---

## ğŸ“š Documentation

### Essential Guides
- **COMPLETE_PROJECT_GUIDE.md** - Full project overview
- **DQN_TRAINING_GUIDE.md** - Detailed training instructions
- **DQN_IMPLEMENTATION_GUIDE.md** - Research paper analysis
- **EVALUATION_RESULTS.md** - BC model performance

### Quick Reference
- **COMMANDS.sh** - All commands in one place
- **QUICK_START.md** - Using the BC model
- **train_dqn.sh** - DQN training launcher

---

## ğŸ” After Training

### 1. Check Results
```bash
# View training summary
cat checkpoints/dqn/training_summary.json

# Best model location
ls -lh checkpoints/dqn/best_model.pt
```

### 2. Evaluate DQN Model
```bash
python3 src/evaluation/evaluate.py \
  --model_path checkpoints/dqn/best_model.pt \
  --test_dir data/processed/test \
  --output_file results/dqn_evaluation.json
```

### 3. Compare with BC
```bash
# BC results
cat results/bc_evaluation.json

# DQN results
cat results/dqn_evaluation.json
```

### 4. View Training Curves
```bash
tensorboard --logdir logs/dqn_self_play
```

---

## ğŸ’¡ Key Features Implemented

### 1. Reward Shaping (from Paper)
```python
# Potential function
Ï•(s) = 0.3*Ï•_land + 0.3*Ï•_army + 0.4*Ï•_castle

# Shaped reward
r = r_base + Î³*Ï•(s') - Ï•(s)
```

Benefits:
- âœ… Dense feedback every step
- âœ… Preserves optimal policies
- âœ… Encourages material advantage

### 2. Self-Play Training
- Agent plays against past versions
- Opponent pool of 3 recent models
- Updates when win rate > 45%

### 3. Double DQN
- Reduces Q-value overestimation
- Uses online network to select actions
- Uses target network to evaluate

### 4. Experience Replay
- Buffer size: 100,000 transitions
- Batch size: 128
- Breaks temporal correlation

### 5. Target Network
- Soft updates (Ï„=0.001)
- Stabilizes training
- Prevents divergence

---

## âš™ï¸ Configuration

All in `src/config.py`:

```python
# Learning
DQN_LEARNING_RATE = 1e-4
DQN_GAMMA = 0.99
DQN_BATCH_SIZE = 128

# Memory
DQN_BUFFER_SIZE = 100000
DQN_MIN_BUFFER_SIZE = 1000

# Exploration
EPSILON_START = 1.0
EPSILON_END = 0.1
EPSILON_DECAY = 0.999

# Updates
DQN_TARGET_UPDATE_FREQUENCY = 100
DQN_TAU = 0.001
```

---

## ğŸ› ï¸ Troubleshooting

### Training Too Slow?
- Reduce `--training_hours` to 2-4
- Or decrease batch size in config

### Loss Not Decreasing?
- Check BC model loaded: "Loading BC checkpoint..."
- Reduce learning rate to 5e-5
- Increase min buffer size to 5000

### Accuracy Not Improving?
- Train longer (6+ hours recommended)
- Check TensorBoard curves
- Verify validation set quality

### Out of Memory?
- Reduce `DQN_BATCH_SIZE` to 64
- Reduce `DQN_BUFFER_SIZE` to 50000
- Close other applications

---

## ğŸ“– Research Background

Based on: "Artificial Generals Intelligence: Mastering Generals.io with RL"
- **Authors:** Matej Straka, Martin Schmid
- **arXiv:** 2507.06825v2
- **GitHub:** https://github.com/strakam/generals-bots

**Paper Results:**
- Method: BC (3h) + PPO (36h) on H100 GPU
- Achievement: Top 0.003% human leaderboard
- Elo: 2052 (vs 2018 baseline)

**Our Approach:**
- Method: BC (14min) + DQN (6h) on M4 Mac
- Expected: Top 1-5% performance
- Elo: ~2000 (vs 1874 baseline)

**Key Differences:**
- âœ… Simpler algorithm (DQN vs PPO)
- âœ… Less training time (6h vs 36h)
- âœ… Consumer hardware (Mac vs H100)
- âœ… Still expect strong results!

---

## ğŸ“ What You'll Learn

By running DQN training, you'll experience:

1. **Self-Play RL:** How agents improve by playing themselves
2. **Reward Shaping:** Guiding learning with domain knowledge
3. **Double DQN:** Advanced Q-learning techniques
4. **Opponent Pools:** Managing training diversity
5. **Long Training:** Monitoring multi-hour runs

---

## âœ… Checklist

Before starting:
- âœ… BC model trained (`checkpoints/bc/best_model.pt` exists)
- âœ… Data preprocessed (48,723 examples)
- âœ… DQN script created (`src/training/train_dqn_self_play.py`)
- âœ… Configuration set (`src/config.py`)
- âœ… Documentation ready (all guides created)

You're all set! ğŸš€

---

## ğŸ¯ START NOW

```bash
# Full 6-hour training (recommended)
./train_dqn.sh 6

# Or quick 3-minute test
./train_dqn.sh 0.05
```

### What to Expect

**Training will:**
- Start with BC model (27.75% accuracy)
- Show progress every 10 episodes
- Evaluate every 50 episodes
- Save best model automatically
- Complete after specified hours

**You'll see:**
- Loss decreasing
- Rewards increasing
- Accuracy improving
- Opponent pool growing

**After 6 hours:**
- Final model saved
- ~32-35% test accuracy
- ~2000 Elo rating
- Ready to deploy!

---

## ğŸ“ Need Help?

1. **Quick commands:** `./COMMANDS.sh`
2. **Training guide:** `DQN_TRAINING_GUIDE.md`
3. **Full overview:** `COMPLETE_PROJECT_GUIDE.md`
4. **Paper details:** `DQN_IMPLEMENTATION_GUIDE.md`

---

## ğŸ‰ Ready!

Your project is complete and ready for DQN training!

**Next command:**
```bash
./train_dqn.sh 6
```

Good luck! ğŸš€ğŸ®

---

*Generated: December 6, 2024*  
*Status: âœ… Ready to train*  
*Expected completion: 6 hours from start*
